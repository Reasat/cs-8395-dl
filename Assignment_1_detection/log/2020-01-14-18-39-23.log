2020-01-14-18-39-23
train_hm.py
--------------------------------------------------------------------------------------------------------------------
agg: sum
alpha: 2.0
batchSize: 16
beta: 4
bottleneckFeatures: 0
dir_lf: D:\Data\cs-8395-dl
dir_project: C:\Users\Reasat\Projects\cs-8395-dl\Assignment_1_detection
encoder: unet-vgg11
epoch: 200
folderData: assignment1_data
lr: 0.001
resume_from: None
--------------------------------------------------------------------------------------------------------------------
creating directory to save model at D:\Data\cs-8395-dl\model\2020-01-14-18-39-23
loading images to RAM
train samples 105
loading images to RAM
validation samples 10
using pretrained (ImageNet) vgg11 as encoder
UNet11(
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace)
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU(inplace)
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace)
    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace)
    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): ReLU(inplace)
    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (14): ReLU(inplace)
    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): ReLU(inplace)
    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (relu): ReLU(inplace)
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3s): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv4s): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv5s): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (center): DecoderBlock(
    (block): Sequential(
      (0): ConvRelu(
        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activation): ReLU(inplace)
      )
      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (2): ReLU(inplace)
    )
  )
  (dec5): DecoderBlock(
    (block): Sequential(
      (0): ConvRelu(
        (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activation): ReLU(inplace)
      )
      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (2): ReLU(inplace)
    )
  )
  (dec4): DecoderBlock(
    (block): Sequential(
      (0): ConvRelu(
        (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activation): ReLU(inplace)
      )
      (1): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (2): ReLU(inplace)
    )
  )
  (dec3): DecoderBlock(
    (block): Sequential(
      (0): ConvRelu(
        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activation): ReLU(inplace)
      )
      (1): ConvTranspose2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (2): ReLU(inplace)
    )
  )
  (dec2): DecoderBlock(
    (block): Sequential(
      (0): ConvRelu(
        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activation): ReLU(inplace)
      )
      (1): ConvTranspose2d(128, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
      (2): ReLU(inplace)
    )
  )
  (dec1): ConvRelu(
    (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (activation): ReLU(inplace)
  )
  (final): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))
)
train >>> epoch: 1/200, batch: 1/7, mean_loss: 3431028.1839
train >>> epoch: 1/200, batch: 2/7, mean_loss: 2844619.2369
train >>> epoch: 1/200, batch: 3/7, mean_loss: 2524948.8557
train >>> epoch: 1/200, batch: 4/7, mean_loss: 2363954.7798
train >>> epoch: 1/200, batch: 5/7, mean_loss: 2266592.0340
train >>> epoch: 1/200, batch: 6/7, mean_loss: 2202616.8512
train >>> epoch: 1/200, batch: 7/7, mean_loss: 1972910.8491
valid >>> epoch: 1/200, mean_loss: 0.1123
criteria decreased from inf to 0.1123, saving best model at D:\Data\cs-8395-dl\model\2020-01-14-18-39-23\2020-01-14-18-39-23_unet-vgg11_best.pt
train >>> epoch: 2/200, batch: 1/7, mean_loss: 1900130.0601
train >>> epoch: 2/200, batch: 2/7, mean_loss: 1893056.5187
train >>> epoch: 2/200, batch: 3/7, mean_loss: 1891189.8897
train >>> epoch: 2/200, batch: 4/7, mean_loss: 1890182.8925
train >>> epoch: 2/200, batch: 5/7, mean_loss: 1888281.8782
train >>> epoch: 2/200, batch: 6/7, mean_loss: 1888046.0935
train >>> epoch: 2/200, batch: 7/7, mean_loss: 1703207.7199
valid >>> epoch: 2/200, mean_loss: 0.1123
train >>> epoch: 3/200, batch: 1/7, mean_loss: 1884804.0537
train >>> epoch: 3/200, batch: 2/7, mean_loss: 1884656.6883
train >>> epoch: 3/200, batch: 3/7, mean_loss: 1881758.5012
train >>> epoch: 3/200, batch: 4/7, mean_loss: 1882151.4757
train >>> epoch: 3/200, batch: 5/7, mean_loss: 1883448.2916
train >>> epoch: 3/200, batch: 6/7, mean_loss: 1883576.0083
train >>> epoch: 3/200, batch: 7/7, mean_loss: 1701531.4379
valid >>> epoch: 3/200, mean_loss: 0.1123
train >>> epoch: 4/200, batch: 1/7, mean_loss: 1881562.0139
train >>> epoch: 4/200, batch: 2/7, mean_loss: 1885982.9773
train >>> epoch: 4/200, batch: 3/7, mean_loss: 1892368.8133
train >>> epoch: 4/200, batch: 4/7, mean_loss: 1890919.7197
train >>> epoch: 4/200, batch: 5/7, mean_loss: 1888753.4477
train >>> epoch: 4/200, batch: 6/7, mean_loss: 1887063.6572
train >>> epoch: 4/200, batch: 7/7, mean_loss: 1702839.3063
valid >>> epoch: 4/200, mean_loss: 0.1123
train >>> epoch: 5/200, batch: 1/7, mean_loss: 1877730.5123
train >>> epoch: 5/200, batch: 2/7, mean_loss: 1892172.3260
train >>> epoch: 5/200, batch: 3/7, mean_loss: 1888439.0681
train >>> epoch: 5/200, batch: 4/7, mean_loss: 1887751.3626
train >>> epoch: 5/200, batch: 5/7, mean_loss: 1888989.2324
train >>> epoch: 5/200, batch: 6/7, mean_loss: 1887260.1445
train >>> epoch: 5/200, batch: 7/7, mean_loss: 1702912.9890
valid >>> epoch: 5/200, mean_loss: 0.1123
train >>> epoch: 6/200, batch: 1/7, mean_loss: 1880088.3595
train >>> epoch: 6/200, batch: 2/7, mean_loss: 1889814.4789
train >>> epoch: 6/200, batch: 3/7, mean_loss: 1885982.9773
train >>> epoch: 6/200, batch: 4/7, mean_loss: 1881930.4275
train >>> epoch: 6/200, batch: 5/7, mean_loss: 1884332.4843
train >>> epoch: 6/200, batch: 6/7, mean_loss: 1887309.2663
train >>> epoch: 6/200, batch: 7/7, mean_loss: 1702931.4097
valid >>> epoch: 6/200, mean_loss: 0.1123
train >>> epoch: 7/200, batch: 1/7, mean_loss: 1890403.9407
train >>> epoch: 7/200, batch: 2/7, mean_loss: 1886867.1700
train >>> epoch: 7/200, batch: 3/7, mean_loss: 1884705.8101
train >>> epoch: 7/200, batch: 4/7, mean_loss: 1885467.1982
train >>> epoch: 7/200, batch: 5/7, mean_loss: 1888635.5553
train >>> epoch: 7/200, batch: 6/7, mean_loss: 1887751.3626
train >>> epoch: 7/200, batch: 7/7, mean_loss: 1703097.1958
valid >>> epoch: 7/200, mean_loss: 0.1123
train >>> epoch: 8/200, batch: 1/7, mean_loss: 1908382.5251
train >>> epoch: 8/200, batch: 2/7, mean_loss: 1899245.8674
train >>> epoch: 8/200, batch: 3/7, mean_loss: 1892467.0569
train >>> epoch: 8/200, batch: 4/7, mean_loss: 1889446.0653
train >>> epoch: 8/200, batch: 5/7, mean_loss: 1887810.3088
train >>> epoch: 8/200, batch: 6/7, mean_loss: 1887849.6063
train >>> epoch: 8/200, batch: 7/7, mean_loss: 1703134.0372
valid >>> epoch: 8/200, mean_loss: 0.1123
train >>> epoch: 9/200, batch: 1/7, mean_loss: 1881562.0139
