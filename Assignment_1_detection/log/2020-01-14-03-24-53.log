2020-01-14-03-24-53
train_hm.py
--------------------------------------------------------------------------------------------------------------------
batchSize: 8
bottleneckFeatures: 0
dir_lf: D:\Data\cs-8395-dl
dir_project: C:\Users\Reasat\Projects\cs-8395-dl\Assignment_1_detection
encoder: hm_net
epoch: 300
folderData: assignment1_data
lr: 0.001
resume_from: None
--------------------------------------------------------------------------------------------------------------------
creating directory to save model at D:\Data\cs-8395-dl\model\2020-01-14-03-24-53
loading images to RAM
train samples 105
loading images to RAM
validation samples 10
HM_Net(
  (conv1): conv_bn_3x3(
    (block): Sequential(
      (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv2): conv_bn_3x3(
    (block): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv3): conv_bn_3x3(
    (block): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv4): conv_bn_3x3(
    (block): Sequential(
      (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv5): conv_bn_3x3(
    (block): Sequential(
      (0): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)
train >>> epoch: 1/300, batch: 1/14, mean_loss: 722212.9913
train >>> epoch: 1/300, batch: 2/14, mean_loss: 721190.8483
train >>> epoch: 1/300, batch: 3/14, mean_loss: 720194.9144
train >>> epoch: 1/300, batch: 4/14, mean_loss: 719153.3958
train >>> epoch: 1/300, batch: 5/14, mean_loss: 718081.2836
train >>> epoch: 1/300, batch: 6/14, mean_loss: 717001.4182
train >>> epoch: 1/300, batch: 7/14, mean_loss: 715917.2794
train >>> epoch: 1/300, batch: 8/14, mean_loss: 714796.2687
train >>> epoch: 1/300, batch: 9/14, mean_loss: 713627.1108
train >>> epoch: 1/300, batch: 10/14, mean_loss: 712471.4477
train >>> epoch: 1/300, batch: 11/14, mean_loss: 711222.9386
train >>> epoch: 1/300, batch: 12/14, mean_loss: 709979.2244
train >>> epoch: 1/300, batch: 13/14, mean_loss: 708610.1878
train >>> epoch: 1/300, batch: 14/14, mean_loss: 664191.5120
5391.630437952358
1229.8424894620912
valid >>> epoch: 1/300, mean_loss: 3310.7365
criteria decreased from inf to 3310.7365, saving best model at D:\Data\cs-8395-dl\model\2020-01-14-03-24-53\2020-01-14-03-24-53_hm_net_best.pt
train >>> epoch: 2/300, batch: 1/14, mean_loss: 684929.8960
train >>> epoch: 2/300, batch: 2/14, mean_loss: 682709.7578
train >>> epoch: 2/300, batch: 3/14, mean_loss: 682324.2082
train >>> epoch: 2/300, batch: 4/14, mean_loss: 679072.4689
train >>> epoch: 2/300, batch: 5/14, mean_loss: 676180.7856
train >>> epoch: 2/300, batch: 6/14, mean_loss: 672966.1491
train >>> epoch: 2/300, batch: 7/14, mean_loss: 670994.1428
train >>> epoch: 2/300, batch: 8/14, mean_loss: 668380.8784
train >>> epoch: 2/300, batch: 9/14, mean_loss: 666436.1501
train >>> epoch: 2/300, batch: 10/14, mean_loss: 661644.5068
train >>> epoch: 2/300, batch: 11/14, mean_loss: 654283.5912
train >>> epoch: 2/300, batch: 12/14, mean_loss: 645305.0109
train >>> epoch: 2/300, batch: 13/14, mean_loss: 636819.2971
train >>> epoch: 2/300, batch: 14/14, mean_loss: 597094.5398
3702.1018823402273
925.1662863513268
valid >>> epoch: 2/300, mean_loss: 2313.6341
criteria decreased from 3310.7365 to 2313.6341, saving best model at D:\Data\cs-8395-dl\model\2020-01-14-03-24-53\2020-01-14-03-24-53_hm_net_best.pt
train >>> epoch: 3/300, batch: 1/14, mean_loss: 524839.6124
train >>> epoch: 3/300, batch: 2/14, mean_loss: 522624.7764
train >>> epoch: 3/300, batch: 3/14, mean_loss: 513363.0246
train >>> epoch: 3/300, batch: 4/14, mean_loss: 447386.1233
train >>> epoch: 3/300, batch: 5/14, mean_loss: 429256.3100
train >>> epoch: 3/300, batch: 6/14, mean_loss: 399937.8747
train >>> epoch: 3/300, batch: 7/14, mean_loss: 379956.6125
train >>> epoch: 3/300, batch: 8/14, mean_loss: 343954.8807
train >>> epoch: 3/300, batch: 9/14, mean_loss: 326971.3181
train >>> epoch: 3/300, batch: 10/14, mean_loss: 296999.8002
train >>> epoch: 3/300, batch: 11/14, mean_loss: 271832.1741
train >>> epoch: 3/300, batch: 12/14, mean_loss: 250556.7626
train >>> epoch: 3/300, batch: 13/14, mean_loss: 233918.8160
train >>> epoch: 3/300, batch: 14/14, mean_loss: 218695.0291
908.8582653533024
227.32171203425582
valid >>> epoch: 3/300, mean_loss: 568.0900
criteria decreased from 2313.6341 to 568.0900, saving best model at D:\Data\cs-8395-dl\model\2020-01-14-03-24-53\2020-01-14-03-24-53_hm_net_best.pt
train >>> epoch: 4/300, batch: 1/14, mean_loss: 772.0374
train >>> epoch: 4/300, batch: 2/14, mean_loss: 8318.6539
train >>> epoch: 4/300, batch: 3/14, mean_loss: 7921.2785
train >>> epoch: 4/300, batch: 4/14, mean_loss: 8321.7587
train >>> epoch: 4/300, batch: 5/14, mean_loss: 6905.1738
train >>> epoch: 4/300, batch: 6/14, mean_loss: 6001.0118
train >>> epoch: 4/300, batch: 7/14, mean_loss: 5892.3180
train >>> epoch: 4/300, batch: 8/14, mean_loss: 5647.4774
